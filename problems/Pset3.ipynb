{
 "metadata": {
  "name": "",
  "signature": "sha256:add933192599142508bc098c99227ab5d321f5ed45dfa2b3a2ef46863a9e8d4f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem 1 (Theoretical tasks)\n",
      "\n",
      "**LU decomposition 4 pts** \n",
      "- Calculate number of operations to get the LU decomposition\n",
      "- How many operations are required to solve a linear system with a matrix given by its LU decomposition?\n",
      "- Is it possible to solve $1\\ 000\\ 000 \\times 1\\ 000\\ 000$ dense linear system on your laptop within 1 month via LU decomposition?\n",
      "\n",
      "**Eigenvalues 4 pts**\n",
      "- Squared matrix $A$ is called diagonalizable if there exists matrix $S$ such that $\\Lambda = S^{-1} A S$  is a diagonal matrix. Prove that if matrix is diagonalizable, then $\\Lambda$ has eigenvalues on a diagonal and $S$ consists of eigenvectors of $A$\n",
      "- Give example of a matrix which can not be diagonalized. What classes of diagonalizable matrices do you know?\n",
      "- Prove that $\\lambda(A) \\in \\mathbb{R}^1$ if $A$ is Hermitian, $\\lambda(A)$ has only imaginary part if $A$ is skew-Hermitian and $\\vert \\lambda(A) \\vert = 1$ if $A$ is unitary\n",
      "\n",
      "**Neumann series 4 pts** \n",
      "- Prove that for every $A$: $\\|A\\| < 1$ holds $$\\|(I - A)^{-1}\\| \\leqslant \\frac{\\|I\\|}{1 - \\|A\\|}$$\n",
      "- Prove that for small perturbation $\\Delta A$ and $\\Delta f$ of $A$ and $f$ in the linear system $Ax = f$ holds\n",
      "$$\\frac{\\Vert \\Delta x \\Vert}{\\Vert x \\Vert} \\leqslant \\frac{\\Vert A \\Vert \\Vert A^{-1} \\Vert}{1 - \\Vert A\\Vert \\Vert A^{-1}\\Vert \\frac{\\Vert\\Delta A\\Vert}{\\Vert A\\Vert}}\\left(\\frac{\\Vert\\Delta A\\Vert}{\\Vert A \\Vert} + \\frac{\\Vert \\Delta f \\Vert}{ \\Vert f \\Vert}\\right)$$\n",
      "What role does $\\|A\\|\\|A^{-1}\\|$ play here?\n",
      "\n",
      "**SVD tool 4 pts** \n",
      "- Prove that $\\|A\\|_2 = \\sigma_1(A) \\equiv \\sqrt{\\lambda_\\text{max} (A^*A)}$ and $ \\|A\\|_F = (\\sigma_1^2 + \\dots + \\sigma_r^2)^{1/2} $\n",
      "- Find optimal constants $c_1$ and $c_2$ such that $c_1 \\|A\\|_2 \\leqslant \\|A\\|_F \\leqslant c_2 \\|A\\|_2$ (matrix norm equivalence)\n",
      "- What is the distance between nonsingular matrix A and the nearest (in terms of the second norm) singular?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem 2 (PageRank)\n",
      "\n",
      "**Connected graph 5 pts**\n",
      "- Create an $n\\times n$ (n = 100) matrix which corresponds to a random connected. The graph should correspond to the PageRank model\n",
      "- Implement power method and find PageRank of the resulting graph. Plot convergence rate of the method\n",
      "\n",
      "**Disconnected graph 6 pts**\n",
      "- Create an $n\\times n$ (n = 200) matrix which corresponds to a random disconnected graph that consists of two connected graphs for PageRank. What is the multiplicity of the eigenvalue $\\lambda = 1$?\n",
      "- To avoid $\\lambda = 1$ degeneration regularize the obtained matrix by multipling it by $1-d$ and by adding $\\verb|d/n * ones((n,n))|$, where $d$ is a small parameter ($\\sim 0.15$) and $n\\times n$ is a size of the graph matrix\n",
      "- Find PageRank via power method. Plot convergence rate of the method for different $d$. Give comments on the convergence\n",
      "\n",
      "<img src=\"http://www.webseoanalytics.com/blog/wp-content/uploads/2010/11/wikipedia-pagerank-nofollow-backlink.jpg\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem 3 (Eigenfaces) 12 pts\n",
      "The aim of this task is to build a face classifier. There are 40 persons in the database. Every person is represented by 10 photos with slightly different facial expression. \n",
      "- Download the database of faces [from here](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)\n",
      "\n",
      "- Create training sample.\n",
      "\n",
      "  Import first 9 images for each face ($9\\times 40$ images). Represent these pictures as a matrix $F$ with $9\\times 40$ columns, where each column is a reshaped 2D picture. Note: use $\\verb|np.reshape|$ to reshape matrix into column\n",
      "  \n",
      "  \n",
      "- Calculate and plot mean face. Subtract it from each column of the matrix $F$\n",
      "\n",
      "- Calculate SVD decomposition of the shifted matrix F and truncate the obtained representaition with first $r$ columns: $U_r S_r V_r^T$. \n",
      "\n",
      " Here $U_r$ is a matrix with $r$ columns - basis set in a space of faces. $W_r = S_r V_r^T$ is a matrix of coefficients in the basis $U_r$. Note that rank $r$ is a parameter which controls quality of classification. Note: parameter $\\verb|full_matrices|$ in $\\verb|np.linalg.svd|$ might be helpful\n",
      " \n",
      " \n",
      "- Plot $U_r$ vectors. Make sure to get face-like images. Remark: now you know what eigenfaces are =)\n",
      "\n",
      "- Import testing set which is the rest of photos. Find their coefficients in the basis $U_r$\n",
      "\n",
      "- Compare the calculated vectors of coefficients to vectors in $W_r$ and classify testing faces. As an output give indices of faces that were misclassified.\n",
      "\n",
      " Similarity of faces is defined by their coefficients similarity which in turn is measured by angle."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Problem 4 (HOSVD) bonus task\n",
      "\n",
      "Implement High-Order SVD (HOSVD) algorithm in 3D. As an output give ranks of the 3D Hilbert tensor $$a_{ijk} = \\frac{1}{i+j+k + 1}, \\quad i,j,k = \\overline{0,199}$$ with $10^{-5}$ accuracy. Details can be found [here (Fig. 4.3)](http://titan.fsb.hr/~venovako/dist/tensor.pdf)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "a = np.random.random((200,200))\n",
      "%timeit np.linalg.svd(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 10.7 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "2./100**3*14000**3/1000/60"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "91.46666666666667"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}